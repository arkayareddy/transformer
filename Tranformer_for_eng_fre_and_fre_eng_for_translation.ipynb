{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer for Next Character Prediction"
      ],
      "metadata": {
        "id": "OXgbDblJVKS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the libraries"
      ],
      "metadata": {
        "id": "1nJ8moIqVQE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries and modules for working with neural networks, numerical operations,\n",
        "# optimization, data handling, and visualization.\n",
        "import functools\n",
        "import flax.linen as nn        # Flax library for neural network models\n",
        "import jax                     # Library for high-performance numerical computing\n",
        "import jax.numpy as jnp        # JAX version of NumPy for array operations\n",
        "from matplotlib import pyplot as plt  # Library for plotting and visualization\n",
        "import optax                   # Optimization library compatible with JAX\n",
        "import tensorflow_datasets as tfds  # Library for accessing pre-built datasets\n",
        "\n",
        "# After importing, check the platform on which JAX is running.\n",
        "# This can help identify whether JAX is utilizing CPU, GPU, or TPU, which is critical\n",
        "# for understanding performance characteristics and optimizations.\n",
        "print(\"JAX running on\", jax.devices()[0].platform.upper())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlXDh0iJ74EU",
        "outputId": "f4731369-26b3-4781-9da3-2e9080353bff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX running on GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameter settings"
      ],
      "metadata": {
        "id": "7G9VLEj8GN7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Random seed:\n",
        "SEED = 42  # @param{type:\"integer\"}\n",
        "# @markdown Learning rate passed to the optimizer:\n",
        "LEARNING_RATE = 5e-3 # @param{type:\"number\"}\n",
        "# @markdown Batch size:\n",
        "BATCH_SIZE = 128  # @param{type:\"integer\"}\n",
        "# @markdown Numer of training iterations:\n",
        "N_ITERATIONS = 6_000  # @param{type:\"integer\"}\n",
        "# @markdown Number of training iterations between two consecutive evaluations:\n",
        "N_FREQ_EVAL = 2_000 # @param{type:\"integer\"}\n",
        "# @markdown Batch size\n",
        "BATCH_SIZE = 512  # @param{type:\"integer\"}\n",
        "# @markdown Rate for dropout in the transformer model\n",
        "DROPOUT_RATE = 0.2  # @param{type:\"number\"}\n",
        "# @markdown Context window for the transformer model\n",
        "BLOCK_SIZE = 64  # @param{type:\"integer\"}\n",
        "# @markdown Number of layer for the transformer model\n",
        "NUM_LAYERS = 6  # @param{type:\"integer\"}\n",
        "# @markdown Size of the embedding for the transformer model\n",
        "EMBED_SIZE = 256  # @param{type:\"integer\"}\n",
        "# @markdown Number of heads for the transformer model\n",
        "NUM_HEADS = 8  # @param{type:\"integer\"}\n",
        "# @markdown Size of the heads for the transformer model\n",
        "HEAD_SIZE = 32  # @param{type:\"integer\"}"
      ],
      "metadata": {
        "id": "5O-YPs_u72RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the data"
      ],
      "metadata": {
        "id": "owfjKGia9oym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = tfds.load(\"tiny_shakespeare\")\n",
        "\n",
        "# combine train and test examples into a single string\n",
        "text_train = \"\"\n",
        "for example in ds[\"train\"].concatenate(ds[\"test\"]).as_numpy_iterator():\n",
        "  text_train += example[\"text\"].decode(\"utf-8\")\n",
        "\n",
        "# similarly, create a single string for validation\n",
        "text_validation = \"\"\n",
        "for example in ds[\"validation\"].as_numpy_iterator():\n",
        "  text_validation += example[\"text\"].decode(\"utf-8\")"
      ],
      "metadata": {
        "id": "ARisZu3t8BnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the length of train and validation data"
      ],
      "metadata": {
        "id": "okY4QeNU9rDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length of text for training: {len(text_train):_} characters\")\n",
        "print(f\"Length of text for validation: {len(text_validation):_} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1DP5ZXs8C2L",
        "outputId": "5a0f2779-1cca-4f7a-e09b-3ab3c27eb942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text for training: 1_059_624 characters\n",
            "Length of text for validation: 55_770 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample train data"
      ],
      "metadata": {
        "id": "3A7b7x3G933T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# small sample of the train set\n",
        "print(text_train[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POzmyna68CoI",
        "outputId": "5ec8bc68-47cf-492e-aee8-3a28f2340c91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vocabulary of the entire data"
      ],
      "metadata": {
        "id": "XnB1ViBS98j2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(list(set(text_train)))\n",
        "print(\"Vocabulary:, \", \"\".join(vocab))\n",
        "print(\"Length of vocabulary: \", len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMZCenbx8Ckw",
        "outputId": "22f6c4c7-8174-45b2-d599-fad741fa57af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:,  \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "Length of vocabulary:  65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vocabulary mapping, encoding and decoding of the data"
      ],
      "metadata": {
        "id": "xH1jK1sW_OfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create mappings from characters to integers and vice versa using a given vocabulary.\n",
        "# `vocab` is assumed to be an iterable of unique characters.\n",
        "stoi = {ch: i for i, ch in enumerate(vocab)}  # Character to index mapping\n",
        "itos = {i: ch for i, ch in enumerate(vocab)}  # Index to character mapping\n",
        "\n",
        "# Define an encoder function that converts a string into a list of integers based on the stoi mapping.\n",
        "encode = lambda s: [\n",
        "    stoi[c] for c in s\n",
        "]  # Encoder: takes a string, outputs a list of integers\n",
        "\n",
        "# Define a decoder function that converts a list of integers back into a string using the itos mapping.\n",
        "decode = lambda l: \"\".join(\n",
        "    [itos[i] for i in l]\n",
        ")  # Decoder: takes a list of integers, outputs a string\n",
        "\n",
        "# Encode train and validation data. This converts all characters in the training and validation text\n",
        "# into their corresponding integer indices based on the vocabulary mapping.\n",
        "# `text_train` and `text_validation` should be strings or lists of characters.\n",
        "train_data = jnp.array(encode(text_train))  # Convert encoded training data into a JAX array\n",
        "eval_data = jnp.array(encode(text_validation))  # Convert encoded validation data into a JAX array"
      ],
      "metadata": {
        "id": "sMUUF0OK8Ch3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batching the data"
      ],
      "metadata": {
        "id": "5mjw_cun_I0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorized mapping of JAX's dynamic slicing to batch multiple slice operations\n",
        "# in a single call. This uses `vmap` to automatically handle batched inputs,\n",
        "# applying `dynamic_slice` across batches.\n",
        "dynamic_slice_vmap = jax.vmap(jax.lax.dynamic_slice, in_axes=(None, 0, None))\n",
        "\n",
        "@jax.jit\n",
        "def get_batch(random_key, data):\n",
        "    \"\"\"\n",
        "    Prepares a random batch of training data using JAX's high-performance operations.\n",
        "    It randomly selects starting indices for sequences within the data, then extracts\n",
        "    those sequences for training and the subsequent sequences as targets.\n",
        "\n",
        "    Args:\n",
        "        random_key (jax.random.PRNGKey): A random seed key used for sampling.\n",
        "        data (array): The complete training dataset stored in a JAX array.\n",
        "\n",
        "    Returns:\n",
        "        x (array): Input sequences extracted from the data.\n",
        "        y (array): Target sequences, each is the subsequent sequence to the corresponding input.\n",
        "    \"\"\"\n",
        "    # Generate random starting indices for data slices.\n",
        "    # `BATCH_SIZE` determines how many sequences to sample, and `BLOCK_SIZE` defines the length of each sequence.\n",
        "    ix = jax.random.randint(\n",
        "        random_key, shape=(BATCH_SIZE, 1), minval=0, maxval=len(data) - BLOCK_SIZE\n",
        "    )\n",
        "\n",
        "    # Fetch the input sequences based on generated indices.\n",
        "    # `x` will be a batch of sequences starting from each index `ix`.\n",
        "    x = dynamic_slice_vmap(data, ix, (BLOCK_SIZE,))\n",
        "\n",
        "    # Fetch the target sequences starting from one position after each `ix` to capture the subsequent values.\n",
        "    # `y` is essentially `x` shifted by one position in the dataset, used for predicting the next item in the sequence.\n",
        "    y = dynamic_slice_vmap(data, ix + 1, (BLOCK_SIZE,))\n",
        "\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "RiLYw4xD8N7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Model"
      ],
      "metadata": {
        "id": "d7t-ZMgZ_DM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NanoLM(nn.Module):\n",
        "  \"\"\"NanoLM model.\"\"\"\n",
        "  vocab_size: int\n",
        "  num_layers: int = 6\n",
        "  num_heads: int = 8\n",
        "  head_size: int = 32\n",
        "  dropout_rate: float = 0.2\n",
        "  embed_size: int = 256\n",
        "  block_size: int = 64\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, training: bool):\n",
        "    seq_len = x.shape[1]\n",
        "\n",
        "    x = nn.Embed(self.vocab_size, self.embed_size)(x) + nn.Embed(\n",
        "        self.block_size, self.embed_size\n",
        "    )(jnp.arange(seq_len))\n",
        "    for _ in range(self.num_layers):\n",
        "      x_norm = nn.LayerNorm()(x)\n",
        "      x = x + nn.MultiHeadDotProductAttention(\n",
        "          num_heads=self.num_heads,\n",
        "          qkv_features=self.head_size,\n",
        "          out_features=self.head_size * self.num_heads,\n",
        "          dropout_rate=self.dropout_rate,\n",
        "      )(\n",
        "          x_norm,\n",
        "          x_norm,\n",
        "          mask=jnp.tril(jnp.ones((x.shape[-2], x.shape[-2]))),\n",
        "          deterministic=not training,\n",
        "      )\n",
        "\n",
        "      x = x + nn.Sequential([\n",
        "          nn.Dense(4 * self.embed_size),\n",
        "          nn.relu,\n",
        "          nn.Dropout(self.dropout_rate, deterministic=not training),\n",
        "          nn.Dense(self.embed_size),\n",
        "      ])(nn.LayerNorm()(x))\n",
        "\n",
        "    x = nn.LayerNorm()(x)\n",
        "    return nn.Dense(self.vocab_size)(x)\n",
        "\n",
        "  @functools.partial(jax.jit, static_argnames=(\"self\", \"length\"))\n",
        "  def generate(self, rng, params, length):\n",
        "    def _scan_generate(carry, _):\n",
        "      random_key, context = carry\n",
        "      logits = self.apply(params, context, training=False)\n",
        "      rng, rng_subkey = jax.random.split(random_key)\n",
        "      new_token = jax.random.categorical(\n",
        "          rng_subkey, logits[:, -1, :], axis=-1, shape=(1, 1)\n",
        "      )\n",
        "      context = jnp.concatenate([context[:, 1:], new_token], axis=1)\n",
        "      return (rng, context), new_token\n",
        "\n",
        "    _, new_tokens = jax.lax.scan(\n",
        "        _scan_generate,\n",
        "        (rng, jnp.zeros((1, self.block_size), dtype=jnp.int32)),\n",
        "        (),\n",
        "        length=length,\n",
        "    )\n",
        "    return new_tokens"
      ],
      "metadata": {
        "id": "cnLzn6938N4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss function and eval step"
      ],
      "metadata": {
        "id": "s4FTdao4Efle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the NanoLM class with specific configuration parameters.\n",
        "# This setup includes specifying the vocabulary size and various architectural details\n",
        "# such as the number of layers, attention heads, and the dropout rate, among others.\n",
        "model = NanoLM(\n",
        "    vocab_size=len(vocab),       # The size of the vocabulary used in the model.\n",
        "    num_layers=NUM_LAYERS,       # The number of transformer layers.\n",
        "    num_heads=NUM_HEADS,         # The number of attention heads in each multi-head attention layer.\n",
        "    head_size=HEAD_SIZE,         # The dimensionality of each attention head.\n",
        "    dropout_rate=DROPOUT_RATE,   # The dropout rate used to prevent overfitting.\n",
        "    embed_size=EMBED_SIZE,       # The size of the embedding layer.\n",
        "    block_size=BLOCK_SIZE,       # The size of the input sequences (block size).\n",
        ")\n",
        "\n",
        "# Defines the loss function used during training. This function calculates\n",
        "# the mean softmax cross-entropy loss between the logits (model outputs) and\n",
        "# the true labels (y), which is a common choice for classification tasks.\n",
        "def loss_fun(params, x, y, dropout_key):\n",
        "  # Apply the model to input x with parameters 'params', enabling dropout\n",
        "  # by passing a dropout RNG key. This is important for training to help with regularization.\n",
        "  logits = model.apply(params, x, training=True, rngs={\"dropout\": dropout_key})\n",
        "  # Calculate the mean cross-entropy loss using Optax's utility function,\n",
        "  # which is suitable for handling logits and integer labels.\n",
        "  return optax.softmax_cross_entropy_with_integer_labels(\n",
        "      logits=logits, labels=y\n",
        "  ).mean()\n",
        "\n",
        "# Defines a function to evaluate the model on a validation or test set.\n",
        "# This is similar to the loss function but with dropout disabled, as indicated\n",
        "# by the 'training=False' argument, which is typical for model evaluation.\n",
        "@jax.jit\n",
        "def eval_step(params, x, y):\n",
        "  # Apply the model to input x with parameters 'params', disabling dropout.\n",
        "  logits = model.apply(params, x, training=False)\n",
        "  # Calculate and return the mean softmax cross-entropy loss.\n",
        "  return optax.softmax_cross_entropy_with_integer_labels(\n",
        "      logits=logits, labels=y\n",
        "  ).mean()"
      ],
      "metadata": {
        "id": "OkQc24Fr8NqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Initilization and seeding for reproducibility"
      ],
      "metadata": {
        "id": "I4684OaHEtMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a PRNGKey with a fixed seed for reproducibility.\n",
        "key = jax.random.PRNGKey(SEED)\n",
        "\n",
        "# Split the PRNGKey to derive a new subkey. This is a common practice in JAX to manage\n",
        "# randomness in a controlled and reproducible manner. The original key is retained\n",
        "# while `subkey` is used for subsequent operations that require randomness.\n",
        "key, subkey = jax.random.split(key)\n",
        "\n",
        "# Initialize the model parameters. The `init` method is used to set up the parameters\n",
        "# of the model based on the shape and type of the input data, and it often requires\n",
        "# a random key to initialize weights in a stochastic manner.\n",
        "var_params = model.init(\n",
        "    key,  # PRNGKey for random initialization of parameters.\n",
        "    jnp.ones((BATCH_SIZE, BLOCK_SIZE), dtype=jnp.int32),  # Dummy input to define the shape and type of model inputs.\n",
        "    training=False,  # Specifies that the model is not in training mode; typically affects certain behaviors like dropout.\n",
        ")"
      ],
      "metadata": {
        "id": "x2P31p_a8Xan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### No of paramaters in the model"
      ],
      "metadata": {
        "id": "VrvHrv4hE2sA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sum the sizes of all parameters in the model. In JAX, model parameters are often organized\n",
        "# in a nested structure. `jax.tree_util.tree_leaves` is used to flatten this structure into\n",
        "# a list of arrays (leaves), where each array represents a parameter (e.g., a weight matrix\n",
        "# or a bias vector).\n",
        "n_params = sum(p.size for p in jax.tree_util.tree_leaves(var_params))\n",
        "\n",
        "# Print the total number of parameters in the model. The formatting option `:_` is used\n",
        "# to separate thousands using underscores for better readability.\n",
        "print(f\"Total number of parameters: {n_params:_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pxy40BwG8XXK",
        "outputId": "7fee5f1f-7c19-4d72-c1fd-a69344d6f805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 3_408_513\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer"
      ],
      "metadata": {
        "id": "WFK3aAb1E6nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To run with SGD instead of adam, replace `adam` with `sgd`\n",
        "opt = optax.adamw(learning_rate=LEARNING_RATE)\n",
        "\n",
        "opt_state = opt.init(var_params)"
      ],
      "metadata": {
        "id": "GShRPpvj8XK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Evaluation"
      ],
      "metadata": {
        "id": "iqyqflyTFWQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Initialize lists to store the training and evaluation losses\n",
        "all_train_losses = []\n",
        "all_eval_losses = []\n",
        "\n",
        "# Define a function 'step' that performs a single optimization step.\n",
        "# This function is decorated with @jax.jit to just-in-time compile it,\n",
        "# significantly speeding up its execution by optimizing the computation graph.\n",
        "@jax.jit\n",
        "def step(key, params, opt_state):\n",
        "    # Split the PRNG key for randomness operations in JAX.\n",
        "    key, subkey = jax.random.split(key)\n",
        "\n",
        "    # Generate a batch of data for training.\n",
        "    batch = get_batch(key, train_data)\n",
        "\n",
        "    # Calculate the loss and its gradients with respect to the parameters.\n",
        "    # `jax.value_and_grad` computes both loss value and gradient in a single function call.\n",
        "    loss, grad = jax.value_and_grad(loss_fun)(params, *batch, subkey)\n",
        "\n",
        "    # Compute parameter updates using the optimizer's update function.\n",
        "    updates, opt_state = opt.update(grad, opt_state, params)\n",
        "\n",
        "    # Apply the updates to the parameters to create new parameters.\n",
        "    params = optax.apply_updates(params, updates)\n",
        "\n",
        "    # Return the new parameters, updated PRNG key, optimizer state, and the loss.\n",
        "    return params, key, opt_state, loss\n",
        "\n",
        "# Iterate over a fixed number of iterations defined by N_ITERATIONS\n",
        "for i in range(N_ITERATIONS):\n",
        "    # Perform an optimization step and update training parameters.\n",
        "    var_params, key, opt_state, loss = step(key, var_params, opt_state)\n",
        "    # Record the training loss\n",
        "    all_train_losses.append(loss)\n",
        "\n",
        "    # Periodically evaluate the model on a validation dataset every N_FREQ_EVAL iterations.\n",
        "    if i % N_FREQ_EVAL == 0:\n",
        "        # Split the PRNG key again for randomness in evaluation.\n",
        "        key, subkey = jax.random.split(key)\n",
        "\n",
        "        # Compute the evaluation loss.\n",
        "        eval_loss = eval_step(var_params, *get_batch(subkey, eval_data))\n",
        "        # Record the evaluation loss.\n",
        "        all_eval_losses.append(eval_loss)\n",
        "\n",
        "        # Print the current step, training loss, and evaluation loss.\n",
        "        print(f\"Step: {i}\\t train loss: {loss}\\t eval loss: {eval_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEdPn0fC8dEE",
        "outputId": "dc43e03d-e084-48fe-ce44-473b5eb168b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 0\t train loss: 4.587119102478027\t eval loss: 6.042149543762207\n",
            "Step: 2000\t train loss: 1.388554573059082\t eval loss: 1.4247114658355713\n",
            "Step: 4000\t train loss: 1.2833781242370605\t eval loss: 1.3967227935791016\n",
            "CPU times: user 4min 16s, sys: 3min 41s, total: 7min 57s\n",
            "Wall time: 7min 52s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample output"
      ],
      "metadata": {
        "id": "FvRDlvF6FawP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now generate some text\n",
        "# Split the PRNG key to obtain a new subkey for generating text. This ensures that randomness in the generation process doesn't affect other random operations.\n",
        "key, subkey = jax.random.split(key)\n",
        "\n",
        "# Generate text using the model. We specify the length of text to generate (1000 characters, words, or tokens, depending on the model's configuration).\n",
        "# The key and parameters ('var_params') are passed to the model's generate function.\n",
        "# The result is reshaped if necessary, here assuming the output is multidimensional where the actual text is in specific dimensions.\n",
        "text = model.generate(subkey, var_params, 1000)[:, 0, 0].tolist()\n",
        "\n",
        "# Decode the generated integer tokens back into human-readable text.\n",
        "# 'decode' is assumed to be a function that maps integer tokens back to strings (e.g., decoding indices to words or characters).\n",
        "print(decode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA98cxv_8jZj",
        "outputId": "a66b312d-e433-408b-be5e-8de35945d020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CAMILLO:\n",
            "Yes, sir, bestrictle.\n",
            "\n",
            "LEONTES:\n",
            "For which I can scoffer.\n",
            "\n",
            "Clown:\n",
            "I shall be full of bosoms in this grief,\n",
            "And throw the droping sords give thee this banish'd:\n",
            "The murderer whereof he's slain are so\n",
            "At gates; the dread ham, on catched of wrongs:\n",
            "That I trew my words from heaven's father,\n",
            "His hug-lips in so face, if his world,\n",
            "Made it kindly know what he most\n",
            "Covenance on their lips I for link not\n",
            "To our great speech.\n",
            "\n",
            "Messenger:\n",
            "My gracious liege, and then I love thee sir;\n",
            "Come about the conspiracy. Thus thus they are the\n",
            "vice music ere thou shalt not abuse thy head?\n",
            "Be thy departing, but at night winter peace,\n",
            "Should be welcome; thy life, thy trembling peace,\n",
            "Comes, give me down as the new-day.\n",
            "\n",
            "Provost:\n",
            "Pardon me, cousin!\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "The princes shall be gone.\n",
            "\n",
            "Provost:\n",
            "I have rather since, the drawbring state\n",
            "of eldest faces is but within such same long.\n",
            "\n",
            "OXTON:\n",
            "Arise, for that yet did follow all,\n",
            "Since yet over--Pluck the action! O, not some other\n",
            "the present. Where\n",
            "Me\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hy01_Ude8jGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers for Translation"
      ],
      "metadata": {
        "id": "XJLvVnnBQTey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Necessary Installations"
      ],
      "metadata": {
        "id": "p2y3D9MPUK_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download fr_core_news_sm\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czbHHQmp9tNt",
        "outputId": "9a635a64-ffd9-4168-ebcd-4267a06b3e59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fr-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from fr-core-news-sm==3.7.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.1.5)\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the libraries"
      ],
      "metadata": {
        "id": "zVyxiJ5yUUa8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3L3RhSQ9DDE",
        "outputId": "7b4e6594-7e1a-49e3-88fd-593cfdb4c235"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import torchtext\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import vocab\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import Tensor\n",
        "from torch.nn import (TransformerEncoder, TransformerDecoder,TransformerEncoderLayer, TransformerDecoderLayer)\n",
        "import io\n",
        "import time\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fre-Eng"
      ],
      "metadata": {
        "id": "ZcXt-Pu4UPNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data loading"
      ],
      "metadata": {
        "id": "H_Ocf3rXVWsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
        "train_urls = ('train.fr.gz', 'train.en.gz')\n",
        "val_urls = ('val.fr.gz', 'val.en.gz')\n",
        "test_urls = ('test_2016_flickr.fr.gz', 'test_2016_flickr.en.gz')\n",
        "\n",
        "# Extracting and downloading data from URLs and obtaining file paths\n",
        "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
        "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
        "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n",
        "\n",
        "# Tokenizers for French and English text\n",
        "fr_tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')\n",
        "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "def build_vocab(filepath, tokenizer):\n",
        "    \"\"\"\n",
        "    Builds a vocabulary from a given file using the provided tokenizer.\n",
        "\n",
        "    Args:\n",
        "    - filepath (str): The path to the file containing text data.\n",
        "    - tokenizer: A tokenizer function capable of tokenizing strings.\n",
        "\n",
        "    Returns:\n",
        "    - vocab: A vocabulary object containing tokens and their frequencies, with special tokens added.\n",
        "    \"\"\"\n",
        "    counter = Counter()  # Initialize a counter to store token frequencies\n",
        "    with io.open(filepath, encoding=\"utf8\") as f:\n",
        "        for string_ in f:  # Iterate through each line in the file\n",
        "            counter.update(tokenizer(string_))  # Tokenize the string and update token frequencies\n",
        "    # Create a vocabulary object with tokens and their frequencies, including special tokens\n",
        "    return vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "\n",
        "# Building vocabularies for French and English using training data\n",
        "fr_vocab = build_vocab(train_filepaths[0], fr_tokenizer)\n",
        "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)\n",
        "\n",
        "# Setting default indices for unknown tokens in vocabularies\n",
        "fr_vocab.set_default_index(fr_vocab['<unk>'])\n",
        "en_vocab.set_default_index(en_vocab['<unk>'])"
      ],
      "metadata": {
        "id": "zr4B6xBo9D13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "1DN6rhzJVUME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process data from given filepaths\n",
        "def data_process(filepaths):\n",
        "    \"\"\"\n",
        "    Processes data from the given filepaths for French and English text.\n",
        "\n",
        "    Args:\n",
        "    - filepaths (list): A list containing two file paths, the first for French and the second for English.\n",
        "\n",
        "    Returns:\n",
        "    - data (list): A list of tuples, each containing processed French and English tensors.\n",
        "    \"\"\"\n",
        "    raw_fr_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))  # Open file for French data\n",
        "    raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))  # Open file for English data\n",
        "    data = []  # Initialize an empty list to store processed data\n",
        "    # Loop through both French and English iterators simultaneously using zip\n",
        "    for (raw_fr, raw_en) in zip(raw_fr_iter, raw_en_iter):\n",
        "        # Tokenize French text, map tokens to indices in fr_vocab, and create a tensor\n",
        "        fr_tensor_ = torch.tensor([fr_vocab[token] for token in fr_tokenizer(raw_fr.rstrip(\"n\"))],\n",
        "                                  dtype=torch.long)\n",
        "        # Tokenize English text, map tokens to indices in en_vocab, and create a tensor\n",
        "        en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en.rstrip(\"n\"))],\n",
        "                                  dtype=torch.long)\n",
        "        # Append processed French and English tensors to the data list\n",
        "        data.append((fr_tensor_, en_tensor_))\n",
        "    return data  # Return processed data\n",
        "\n",
        "# Process data for training, validation, and testing sets\n",
        "train_data = data_process(train_filepaths)\n",
        "val_data = data_process(val_filepaths)\n",
        "test_data = data_process(test_filepaths)\n",
        "\n",
        "# Set device to GPU if available, otherwise to CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Constants for batch processing\n",
        "BATCH_SIZE = 128  # Batch size for training\n",
        "PAD_IDX = fr_vocab['<pad>'] # Index of the padding token in the French vocabulary\n",
        "BOS_IDX = fr_vocab['<bos>'] # Index of the beginning-of-sequence token in the French vocabulary\n",
        "EOS_IDX = fr_vocab['<eos>'] # Index of the end-of-sequence token in the French vocabulary"
      ],
      "metadata": {
        "id": "jC9LbkLj9Ri-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate data in batches"
      ],
      "metadata": {
        "id": "Y7FwTef8VO04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader\n",
        "def generate_batch(data_batch):\n",
        "    \"\"\"\n",
        "    Generates batches of data with padding for sequences.\n",
        "\n",
        "    Args:\n",
        "    - data_batch (list): A batch of data, where each element is a tuple containing processed French and English tensors.\n",
        "\n",
        "    Returns:\n",
        "    - fr_batch (Tensor): A tensor containing padded sequences of French tensors for the batch.\n",
        "    - en_batch (Tensor): A tensor containing padded sequences of English tensors for the batch.\n",
        "    \"\"\"\n",
        "    fr_batch, en_batch = [], []  # Initialize empty lists for French and English batches\n",
        "    # Iterate through each item in the data batch\n",
        "    for (fr_item, en_item) in data_batch:\n",
        "        # Add beginning-of-sequence and end-of-sequence tokens to French tensors and concatenate them\n",
        "        fr_batch.append(torch.cat([torch.tensor([BOS_IDX]), fr_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "        # Add beginning-of-sequence and end-of-sequence tokens to English tensors and concatenate them\n",
        "        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    # Pad sequences in the French batch with padding value PAD_IDX\n",
        "    fr_batch = pad_sequence(fr_batch, padding_value=PAD_IDX)\n",
        "    # Pad sequences in the English batch with padding value PAD_IDX\n",
        "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
        "    return fr_batch, en_batch  # Return padded French and English batches\n",
        "\n",
        "# DataLoader setup for training, validation, and testing data\n",
        "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
        "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
        "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)"
      ],
      "metadata": {
        "id": "1gBnuP_89cnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Class"
      ],
      "metadata": {
        "id": "PNbEDEzTVL4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer class for sequence-to-sequence model\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer-based sequence-to-sequence model for machine translation.\n",
        "\n",
        "    Args:\n",
        "    - num_encoder_layers (int): Number of layers in the encoder.\n",
        "    - num_decoder_layers (int): Number of layers in the decoder.\n",
        "    - emb_size (int): Embedding size for tokens.\n",
        "    - src_vocab_size (int): Vocabulary size of the source language.\n",
        "    - tgt_vocab_size (int): Vocabulary size of the target language.\n",
        "    - dim_feedforward (int): Dimension of the feedforward network in Transformer layers.\n",
        "    - dropout (float): Dropout probability.\n",
        "\n",
        "    Methods:\n",
        "    - forward(src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n",
        "      Forward pass of the model.\n",
        "    - encode(src, src_mask):\n",
        "      Encoder forward pass.\n",
        "    - decode(tgt, memory, tgt_mask):\n",
        "      Decoder forward pass.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_encoder_layers: int, num_decoder_layers: int,\n",
        "                 emb_size: int, src_vocab_size: int, tgt_vocab_size: int,\n",
        "                 dim_feedforward:int = 512, dropout:float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "\n",
        "        # Transformer encoder\n",
        "        encoder_layer = TransformerEncoderLayer(d_model=emb_size, nhead=NHEAD,\n",
        "                                                dim_feedforward=dim_feedforward)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "\n",
        "        # Transformer decoder\n",
        "        decoder_layer = TransformerDecoderLayer(d_model=emb_size, nhead=NHEAD,\n",
        "                                                dim_feedforward=dim_feedforward)\n",
        "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        # Generator layer\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "\n",
        "        # Token embeddings for source and target languages\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self, src: Tensor, trg: Tensor, src_mask: Tensor,\n",
        "                tgt_mask: Tensor, src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor, memory_key_padding_mask: Tensor):\n",
        "        \"\"\"\n",
        "        Forward pass of the transformer-based sequence-to-sequence model.\n",
        "\n",
        "        Args:\n",
        "        - src (Tensor): Input tensor of shape (src_seq_len, batch_size).\n",
        "        - trg (Tensor): Target tensor of shape (trg_seq_len, batch_size).\n",
        "        - src_mask (Tensor): Mask for source tokens of shape (src_seq_len, src_seq_len).\n",
        "        - tgt_mask (Tensor): Mask for target tokens of shape (trg_seq_len, trg_seq_len).\n",
        "        - src_padding_mask (Tensor): Mask for padding tokens in source of shape (batch_size, src_seq_len).\n",
        "        - tgt_padding_mask (Tensor): Mask for padding tokens in target of shape (batch_size, trg_seq_len).\n",
        "        - memory_key_padding_mask (Tensor): Mask for padding in memory key of shape (batch_size, src_seq_len).\n",
        "\n",
        "        Returns:\n",
        "        - Tensor: Output tensor of shape (trg_seq_len, batch_size, tgt_vocab_size).\n",
        "        \"\"\"\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        memory = self.transformer_encoder(src_emb, src_mask, src_padding_mask)\n",
        "        outs = self.transformer_decoder(tgt_emb, memory, tgt_mask, None,\n",
        "                                        tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        \"\"\"\n",
        "        Encoder forward pass.\n",
        "\n",
        "        Args:\n",
        "        - src (Tensor): Input tensor of shape (src_seq_len, batch_size).\n",
        "        - src_mask (Tensor): Mask for source tokens of shape (src_seq_len, src_seq_len).\n",
        "\n",
        "        Returns:\n",
        "        - Tensor: Encoded tensor of shape (src_seq_len, batch_size, emb_size).\n",
        "        \"\"\"\n",
        "        return self.transformer_encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        \"\"\"\n",
        "        Decoder forward pass.\n",
        "\n",
        "        Args:\n",
        "        - tgt (Tensor): Input tensor of shape (trg_seq_len, batch_size).\n",
        "        - memory (Tensor): Memory tensor from encoder of shape (src_seq_len, batch_size, emb_size).\n",
        "        - tgt_mask (Tensor): Mask for target tokens of shape (trg_seq_len, trg_seq_len).\n",
        "\n",
        "        Returns:\n",
        "        - Tensor: Decoded tensor of shape (trg_seq_len, batch_size, emb_size).\n",
        "        \"\"\"\n",
        "        return self.transformer_decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ],
      "metadata": {
        "id": "w7DaRRc9Bz3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Embedding and Embedding"
      ],
      "metadata": {
        "id": "aNKyF5i9VFNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Positional encoding module for adding positional information to token embeddings.\n",
        "\n",
        "    Args:\n",
        "    - emb_size (int): Embedding size.\n",
        "    - dropout (float): Dropout probability.\n",
        "    - maxlen (int): Maximum sequence length.\n",
        "\n",
        "    Methods:\n",
        "    - forward(token_embedding: Tensor):\n",
        "      Forward pass of the positional encoding module.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_size: int, dropout, maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # Calculate positional encodings\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Register positional embeddings as a buffer\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        \"\"\"\n",
        "        Forward pass of the positional encoding module.\n",
        "\n",
        "        Args:\n",
        "        - token_embedding (Tensor): Token embeddings of shape (seq_len, batch_size, emb_size).\n",
        "\n",
        "        Returns:\n",
        "        - Tensor: Token embeddings with positional encodings added, of shape (seq_len, batch_size, emb_size).\n",
        "        \"\"\"\n",
        "        # Add positional encodings to token embeddings\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0),:])\n",
        "\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Token embedding module for converting token indices into token embeddings.\n",
        "\n",
        "    Args:\n",
        "    - vocab_size (int): Vocabulary size.\n",
        "    - emb_size (int): Embedding size.\n",
        "\n",
        "    Methods:\n",
        "    - forward(tokens: Tensor):\n",
        "      Forward pass of the token embedding module.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        # Initialize an embedding layer for tokens\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        \"\"\"\n",
        "        Forward pass of the token embedding module.\n",
        "\n",
        "        Args:\n",
        "        - tokens (Tensor): Token indices of shape (seq_len, batch_size).\n",
        "\n",
        "        Returns:\n",
        "        - Tensor: Token embeddings of shape (seq_len, batch_size, emb_size).\n",
        "        \"\"\"\n",
        "        # Convert token indices into token embeddings and scale them\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
      ],
      "metadata": {
        "id": "So5AfNTMB-2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Masking"
      ],
      "metadata": {
        "id": "905duNe-VCba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    \"\"\"\n",
        "    Generates a square subsequent mask for self-attention mechanisms in transformer models.\n",
        "\n",
        "    Args:\n",
        "    - sz (int): Size of the square mask.\n",
        "\n",
        "    Returns:\n",
        "    - Tensor: Square subsequent mask of shape (sz, sz).\n",
        "    \"\"\"\n",
        "    # Create an upper triangular matrix of ones\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    # Convert the mask to float and replace zeros with negative infinity and ones with zero\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    \"\"\"\n",
        "    Creates masks for source and target sequences to be used in transformer models.\n",
        "\n",
        "    Args:\n",
        "    - src (Tensor): Source tensor of shape (src_seq_len, batch_size).\n",
        "    - tgt (Tensor): Target tensor of shape (tgt_seq_len, batch_size).\n",
        "\n",
        "    Returns:\n",
        "    - src_mask (Tensor): Mask for source sequence of shape (src_seq_len, src_seq_len).\n",
        "    - tgt_mask (Tensor): Mask for target sequence of shape (tgt_seq_len, tgt_seq_len).\n",
        "    - src_padding_mask (Tensor): Padding mask for source sequence of shape (batch_size, src_seq_len).\n",
        "    - tgt_padding_mask (Tensor): Padding mask for target sequence of shape (batch_size, tgt_seq_len).\n",
        "    \"\"\"\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    # Generate mask for the target sequence\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    # Create a padding mask for the source sequence\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n",
        "\n",
        "    # Create padding masks for both source and target sequences\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "metadata": {
        "id": "ogyzIBJtCYoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Important parmeters"
      ],
      "metadata": {
        "id": "QfPXUiOeU-T2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define vocabulary sizes and model parameters\n",
        "SRC_VOCAB_SIZE = len(fr_vocab)\n",
        "TGT_VOCAB_SIZE = len(en_vocab)\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Determine the device (CPU or GPU) for computation\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Instantiate the Seq2SeqTransformer model\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS,\n",
        "                                 EMB_SIZE, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE,\n",
        "                                 FFN_HID_DIM)\n",
        "\n",
        "# Initialize model parameters using Xavier initialization\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "# Move the model to the selected device\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "# Define the loss function (CrossEntropyLoss) ignoring padding tokens\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "# Define the optimizer (Adam) with specific parameters\n",
        "optimizer = torch.optim.Adam(\n",
        "    transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qh77QP6RCbNt",
        "outputId": "783f37e2-3510-40a9-bdc3-b8da35e96ca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train function"
      ],
      "metadata": {
        "id": "ocWPPKHVU5xZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, train_iter, optimizer, epoch, save_path):\n",
        "    \"\"\"\n",
        "    Trains the model for one epoch using the provided training iterator and optimizer.\n",
        "\n",
        "    Args:\n",
        "    - model (nn.Module): The model to be trained.\n",
        "    - train_iter (DataLoader): The data loader iterator for training data.\n",
        "    - optimizer (torch.optim.Optimizer): The optimizer used for training.\n",
        "    - epoch (int): The current epoch number.\n",
        "    - save_path (str): The directory path to save the trained model.\n",
        "\n",
        "    Returns:\n",
        "    - train_loss (float): The average loss of the model over the train set.\n",
        "    \"\"\"\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loss = 0  # Initialize total loss for the epoch\n",
        "    total_correct = 0  # Initialize total number of correct predictions\n",
        "    total_elements = 0  # Initialize total token count for accuracy calculation\n",
        "\n",
        "    # Iterate over the training data iterator\n",
        "    for idx, (src, tgt) in enumerate(train_iter):\n",
        "        src = src.to(device)  # Move source data to the appropriate device\n",
        "        tgt = tgt.to(device)  # Move target data to the appropriate device\n",
        "        tgt_input = tgt[:-1, :]  # Get input to the decoder (exclude last token)\n",
        "\n",
        "        # Generate masks for source and target sequences\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        # Zero the gradients before backward pass\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Define target output (right-shifted)\n",
        "        tgt_out = tgt[1:, :]\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "\n",
        "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the total loss for the epoch\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate average loss and accuracy for the epoch\n",
        "    train_loss = total_loss / len(train_iter)\n",
        "\n",
        "    # Save the trained model after each epoch\n",
        "    model_save_path = f'{save_path}/model_epoch_{epoch}.pth'\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "    return train_loss"
      ],
      "metadata": {
        "id": "AsT47AnMChMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate function"
      ],
      "metadata": {
        "id": "uaXWvAWgU1xD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, val_iter, device):\n",
        "    \"\"\"\n",
        "    Evaluate the performance of a model on the validation dataset.\n",
        "\n",
        "    This function calculates the average loss and accuracy of the model\n",
        "    over the entire validation set. The model is set to evaluation mode to\n",
        "    disable operations like dropout.\n",
        "\n",
        "    Args:\n",
        "    - model (torch.nn.Module): The model to be evaluated.\n",
        "    - val_iter (iterable): An iterable over the validation dataset. Each iteration\n",
        "                           produces a pair (src, tgt) representing source and target data.\n",
        "    - device (torch.device): The device tensors should be sent to (e.g., 'cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "    - val_loss (float): The average loss of the model over the validation set.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    total_elements = 0\n",
        "\n",
        "    for idx, (src, tgt) in enumerate(val_iter):\n",
        "        src = src.to(device)  # Move source data to the specified device\n",
        "        tgt = tgt.to(device)  # Move target data to the specified device\n",
        "\n",
        "        tgt_input = tgt[:-1, :]  # Prepare the input for the target sequence\n",
        "\n",
        "        # Create masks for the source and target input\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        # Forward pass: compute predicted logits by passing src, tgt_input, and masks to the model\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,\n",
        "                       src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]  # The actual targets, excluding the first element of tgt\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        total_loss += loss.item()  # Accumulate the loss\n",
        "\n",
        "    val_loss = total_loss / len(val_iter)  # Calculate average loss\n",
        "\n",
        "    return val_loss"
      ],
      "metadata": {
        "id": "mZIWF3w2C75f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Val loss"
      ],
      "metadata": {
        "id": "ofNoiJ1lUx6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path where the model or other data might be saved or accessed.\n",
        "path = '/content/drive/MyDrive/trf/'\n",
        "\n",
        "# Loop over each epoch starting from 1 to NUM_EPOCHS\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = time.time()  # Record the start time of the epoch\n",
        "\n",
        "    # Train the model for one epoch and retrieve training loss and accuracy\n",
        "    train_loss = train_epoch(transformer, train_iter, optimizer, epoch, path)\n",
        "\n",
        "    end_time = time.time()  # Record the end time of the training\n",
        "\n",
        "    # Evaluate the model on the validation dataset and retrieve loss and accuracy\n",
        "    val_loss = evaluate(transformer, valid_iter, device)\n",
        "\n",
        "    # Print the results for the epoch including both training and validation metrics\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"\n",
        "           f\"Epoch time = {(end_time - start_time):.3f}s\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAjUQrrZC_gD",
        "outputId": "29a81204-4048-490f-f27a-2f07bd74ebe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train loss: 0.810, Val loss: 1.410, Epoch time = 19.292s\n",
            "Epoch: 2, Train loss: 0.761, Val loss: 1.421, Epoch time = 19.433s\n",
            "Epoch: 3, Train loss: 0.715, Val loss: 1.397, Epoch time = 19.517s\n",
            "Epoch: 4, Train loss: 0.674, Val loss: 1.412, Epoch time = 19.564s\n",
            "Epoch: 5, Train loss: 0.635, Val loss: 1.404, Epoch time = 19.583s\n",
            "Epoch: 6, Train loss: 0.598, Val loss: 1.412, Epoch time = 19.590s\n",
            "Epoch: 7, Train loss: 0.563, Val loss: 1.415, Epoch time = 19.736s\n",
            "Epoch: 8, Train loss: 0.530, Val loss: 1.415, Epoch time = 19.652s\n",
            "Epoch: 9, Train loss: 0.499, Val loss: 1.431, Epoch time = 19.812s\n",
            "Epoch: 10, Train loss: 0.468, Val loss: 1.442, Epoch time = 19.638s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Greedy Decoder\n"
      ],
      "metadata": {
        "id": "8ziWucbOUsnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    \"\"\"\n",
        "    Performs greedy decoding to generate a translation sequence given a source sequence.\n",
        "\n",
        "    Args:\n",
        "    - model (nn.Module): The trained model for sequence-to-sequence translation.\n",
        "    - src (Tensor): Source sequence tensor of shape (src_seq_len, batch_size).\n",
        "    - src_mask (Tensor): Source mask tensor to mask out padding tokens.\n",
        "    - max_len (int): Maximum length of the generated translation sequence.\n",
        "    - start_symbol (int): Index of the start symbol in the target vocabulary.\n",
        "\n",
        "    Returns:\n",
        "    - Tensor: Generated translation sequence tensor of shape (tgt_seq_len, batch_size).\n",
        "    \"\"\"\n",
        "    src = src.to(device)  # Move source tensor to device (CPU or GPU)\n",
        "    src_mask = src_mask.to(device)  # Move source mask tensor to device (CPU or GPU)\n",
        "    memory = model.encode(src, src_mask)  # Encode the source sequence\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)  # Initialize target sequence with start symbol\n",
        "    # Iterate over target sequence until maximum length or end-of-sequence token is reached\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(device)  # Move memory tensor to device (CPU or GPU)\n",
        "        # Create mask for memory tensor to mask out padding tokens\n",
        "        memory_mask = torch.zeros(ys.shape[0], memory.shape[0]).to(device).type(torch.bool)\n",
        "        # Generate mask for target sequence to mask out subsequent tokens\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(device)\n",
        "        # Decode the target sequence\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)  # Transpose output tensor\n",
        "        # Generate probability distribution over target vocabulary for the next word\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)  # Get index of the word with maximum probability\n",
        "        next_word = next_word.item()  # Convert index to Python integer\n",
        "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)  # Append next word to target sequence\n",
        "        if next_word == EOS_IDX:  # Check if end-of-sequence token is reached\n",
        "            break\n",
        "    return ys  # Return the generated translation sequence"
      ],
      "metadata": {
        "id": "4wr7VOTIDCm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translation function"
      ],
      "metadata": {
        "id": "aAt6mVSuUa0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(model, src, src_vocab, tgt_vocab, src_tokenizer):\n",
        "    \"\"\"\n",
        "    Translates a source sentence to the target language using the trained model.\n",
        "\n",
        "    Args:\n",
        "    - model (nn.Module): The trained model for sequence-to-sequence translation.\n",
        "    - src (str): Source sentence to be translated.\n",
        "    - src_vocab (Vocab): Source vocabulary object.\n",
        "    - tgt_vocab (Vocab): Target vocabulary object.\n",
        "    - src_tokenizer (Tokenizer): Tokenizer for the source language.\n",
        "\n",
        "    Returns:\n",
        "    - str: Translated sentence in the target language.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    # Tokenize the source sentence and convert tokens to indices using the source vocabulary\n",
        "    tokens = [BOS_IDX] + [src_vocab.get_stoi()[tok] for tok in src_tokenizer(src)] + [EOS_IDX]\n",
        "    num_tokens = len(tokens)  # Get the number of tokens in the source sentence\n",
        "    src = (torch.LongTensor(tokens).reshape(num_tokens, 1))  # Convert tokens to tensor\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)  # Create source mask tensor\n",
        "    # Generate translation using greedy decoding\n",
        "    tgt_tokens = greedy_decode(model, src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    # Convert target tokens to words using the target vocabulary and join them into a sentence\n",
        "    translation = \" \".join([tgt_vocab.get_itos()[tok] for tok in tgt_tokens]).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
        "    return translation  # Return the translated sentence"
      ],
      "metadata": {
        "id": "PtrgC5GjJEFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample output"
      ],
      "metadata": {
        "id": "c3CTX8_9UfHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Translate a source sentence from French to English\n",
        "output = translate(transformer, \"Un groupe de personnes se tient devant un igloo .\", fr_vocab, en_vocab, fr_tokenizer)\n",
        "# Print the translated output\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njps5qAMJG6c",
        "outputId": "222802f1-2583-4eb1-f722-2758b04cddb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A group of people stand in front of an igloo . \n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Translate a source sentence from French to English\n",
        "output = translate(transformer, \"Il chante dans la chorale .\", fr_vocab, en_vocab, fr_tokenizer)\n",
        "# Print the translated output\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YlAOih2qwp1",
        "outputId": "3da6e3ee-473b-4cd6-851b-7f4e78a483cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " He is singing in the choir . \n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eng-Fre"
      ],
      "metadata": {
        "id": "nBXBzMdLPJ46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Loading"
      ],
      "metadata": {
        "id": "ctS7Z6rYVds8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
        "train_urls = ('train.en.gz', 'train.fr.gz')\n",
        "val_urls = ('val.en.gz', 'val.fr.gz')\n",
        "test_urls = ('test_2016_flickr.en.gz', 'test_2016_flickr.fr.gz')\n",
        "\n",
        "# Extracting and downloading data from URLs and obtaining file paths\n",
        "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
        "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
        "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n",
        "\n",
        "# Tokenizers for French and English text\n",
        "fr_tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')\n",
        "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "def build_vocab(filepath, tokenizer):\n",
        "    \"\"\"\n",
        "    Builds a vocabulary from a given file using the provided tokenizer.\n",
        "\n",
        "    Args:\n",
        "    - filepath (str): The path to the file containing text data.\n",
        "    - tokenizer: A tokenizer function capable of tokenizing strings.\n",
        "\n",
        "    Returns:\n",
        "    - vocab: A vocabulary object containing tokens and their frequencies, with special tokens added.\n",
        "    \"\"\"\n",
        "    counter = Counter()  # Initialize a counter to store token frequencies\n",
        "    with io.open(filepath, encoding=\"utf8\") as f:\n",
        "        for string_ in f:  # Iterate through each line in the file\n",
        "            counter.update(tokenizer(string_))  # Tokenize the string and update token frequencies\n",
        "    # Create a vocabulary object with tokens and their frequencies, including special tokens\n",
        "    return vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "\n",
        "# Building vocabularies for French and English using training data\n",
        "fr_vocab = build_vocab(train_filepaths[1], fr_tokenizer)\n",
        "en_vocab = build_vocab(train_filepaths[0], en_tokenizer)\n",
        "\n",
        "# Setting default indices for unknown tokens in vocabularies\n",
        "fr_vocab.set_default_index(fr_vocab['<unk>'])\n",
        "en_vocab.set_default_index(en_vocab['<unk>'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEVgUarMJI-V",
        "outputId": "701ec030-79cf-4fb6-ba61-0e35083531f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "nr2BC8Cml7pN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_process(filepaths):\n",
        "    \"\"\"\n",
        "    Processes raw text data from filepaths into tensors for both French and English languages.\n",
        "\n",
        "    Args:\n",
        "    - filepaths (list): List of filepaths for the French and English datasets.\n",
        "\n",
        "    Returns:\n",
        "    - list: List of tuples containing processed tensor pairs (English, French).\n",
        "    \"\"\"\n",
        "    raw_fr_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))  # Open French data file for reading\n",
        "    raw_en_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))  # Open English data file for reading\n",
        "    data = []  # Initialize empty list to store processed data\n",
        "    # Iterate over both French and English iterators simultaneously using zip\n",
        "    for (raw_fr, raw_en) in zip(raw_fr_iter, raw_en_iter):\n",
        "        # Tokenize French text, map tokens to indices in fr_vocab, and create a tensor\n",
        "        fr_tensor_ = torch.tensor([fr_vocab[token] for token in fr_tokenizer(raw_fr.rstrip(\"n\"))],\n",
        "                                  dtype=torch.long)\n",
        "        # Tokenize English text, map tokens to indices in en_vocab, and create a tensor\n",
        "        en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en.rstrip(\"n\"))],\n",
        "                                  dtype=torch.long)\n",
        "        # Append processed English and French tensors to the data list\n",
        "        data.append((en_tensor_, fr_tensor_))\n",
        "    return data  # Return processed data\n",
        "\n",
        "# Process data for training, validation, and testing sets\n",
        "train_data = data_process(train_filepaths)\n",
        "val_data = data_process(val_filepaths)\n",
        "test_data = data_process(test_filepaths)\n",
        "\n",
        "# Set device to GPU if available, otherwise to CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Constants for batch processing\n",
        "BATCH_SIZE = 128  # Batch size for training\n",
        "PAD_IDX = fr_vocab['<pad>'] # Index of the padding token in the French vocabulary\n",
        "BOS_IDX = fr_vocab['<bos>'] # Index of the beginning-of-sequence token in the French vocabulary\n",
        "EOS_IDX = fr_vocab['<eos>'] # Index of the end-of-sequence token in the French vocabulary"
      ],
      "metadata": {
        "id": "9W9OboNQP5MJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate data in batches"
      ],
      "metadata": {
        "id": "fyxugLwWmEQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader\n",
        "def generate_batch(data_batch):\n",
        "    \"\"\"\n",
        "    Generates batches of data by padding sequences to the maximum length in the batch.\n",
        "\n",
        "    Args:\n",
        "    - data_batch (list): List of tuples containing paired sequences (English, French).\n",
        "\n",
        "    Returns:\n",
        "    - Tensor: Padded batch of English sequences.\n",
        "    - Tensor: Padded batch of French sequences.\n",
        "    \"\"\"\n",
        "    en_batch, fr_batch = [], []  # Initialize empty lists to store batches of English and French sequences\n",
        "    # Iterate over each tuple (English, French) in the data batch\n",
        "    for (en_item, fr_item) in data_batch:\n",
        "        # Append the English sequence with start-of-sequence and end-of-sequence tokens and concatenate into a tensor\n",
        "        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "        # Append the French sequence with start-of-sequence and end-of-sequence tokens and concatenate into a tensor\n",
        "        fr_batch.append(torch.cat([torch.tensor([BOS_IDX]), fr_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    # Pad sequences in the English batch with the padding token to ensure uniform length\n",
        "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
        "    # Pad sequences in the French batch with the padding token to ensure uniform length\n",
        "    fr_batch = pad_sequence(fr_batch, padding_value=PAD_IDX)\n",
        "    return en_batch, fr_batch  # Return padded batches of English and French sequences\n",
        "\n",
        "\n",
        "# Create data loaders for train, validation, and test datasets using the generated batch function\n",
        "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
        "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
        "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)"
      ],
      "metadata": {
        "id": "LuD6T_iwQN4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Class"
      ],
      "metadata": {
        "id": "3r7Q2phVlw_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer class for sequence-to-sequence model\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer-based sequence-to-sequence model for machine translation.\n",
        "\n",
        "    Args:\n",
        "    - num_encoder_layers (int): Number of layers in the encoder.\n",
        "    - num_decoder_layers (int): Number of layers in the decoder.\n",
        "    - emb_size (int): Embedding size for tokens.\n",
        "    - src_vocab_size (int): Vocabulary size of the source language.\n",
        "    - tgt_vocab_size (int): Vocabulary size of the target language.\n",
        "    - dim_feedforward (int): Dimension of the feedforward network in Transformer layers.\n",
        "    - dropout (float): Dropout probability.\n",
        "\n",
        "    Methods:\n",
        "    - forward(src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n",
        "      Forward pass of the model.\n",
        "    - encode(src, src_mask):\n",
        "      Encoder forward pass.\n",
        "    - decode(tgt, memory, tgt_mask):\n",
        "      Decoder forward pass.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_encoder_layers: int, num_decoder_layers: int,\n",
        "                 emb_size: int, src_vocab_size: int, tgt_vocab_size: int,\n",
        "                 dim_feedforward:int = 512, dropout:float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "\n",
        "        # Transformer encoder\n",
        "        encoder_layer = TransformerEncoderLayer(d_model=emb_size, nhead=NHEAD,\n",
        "                                                dim_feedforward=dim_feedforward)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "\n",
        "        # Transformer decoder\n",
        "        decoder_layer = TransformerDecoderLayer(d_model=emb_size, nhead=NHEAD,\n",
        "                                                dim_feedforward=dim_feedforward)\n",
        "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        # Generator layer\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "\n",
        "        # Token embeddings for source and target languages\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self, src: Tensor, trg: Tensor, src_mask: Tensor,\n",
        "                tgt_mask: Tensor, src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor, memory_key_padding_mask: Tensor):\n",
        "        \"\"\"\n",
        "        Forward pass of the transformer-based sequence-to-sequence model.\n",
        "\n",
        "        Args:\n",
        "        - src (Tensor): Input tensor of shape (src_seq_len, batch_size).\n",
        "        - trg (Tensor): Target tensor of shape (trg_seq_len, batch_size).\n",
        "        - src_mask (Tensor): Mask for source tokens of shape (src_seq_len, src_seq_len).\n",
        "        - tgt_mask (Tensor): Mask for target tokens of shape (trg_seq_len, trg_seq_len).\n",
        "        - src_padding_mask (Tensor): Mask for padding tokens in source of shape (batch_size, src_seq_len).\n",
        "        - tgt_padding_mask (Tensor): Mask for padding tokens in target of shape (batch_size, trg_seq_len).\n",
        "        - memory_key_padding_mask (Tensor): Mask for padding in memory key of shape (batch_size, src_seq_len).\n",
        "\n",
        "        Returns:\n",
        "        - Tensor: Output tensor of shape (trg_seq_len, batch_size, tgt_vocab_size).\n",
        "        \"\"\"\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        memory = self.transformer_encoder(src_emb, src_mask, src_padding_mask)\n",
        "        outs = self.transformer_decoder(tgt_emb, memory, tgt_mask, None,\n",
        "                                        tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        \"\"\"\n",
        "        Encoder forward pass.\n",
        "\n",
        "        Args:\n",
        "        - src (Tensor): Input tensor of shape (src_seq_len, batch_size).\n",
        "        - src_mask (Tensor): Mask for source tokens of shape (src_seq_len, src_seq_len).\n",
        "\n",
        "        Returns:\n",
        "        - Tensor: Encoded tensor of shape (src_seq_len, batch_size, emb_size).\n",
        "        \"\"\"\n",
        "        return self.transformer_encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        \"\"\"\n",
        "        Decoder forward pass.\n",
        "\n",
        "        Args:\n",
        "        - tgt (Tensor): Input tensor of shape (trg_seq_len, batch_size).\n",
        "        - memory (Tensor): Memory tensor from encoder of shape (src_seq_len, batch_size, emb_size).\n",
        "        - tgt_mask (Tensor): Mask for target tokens of shape (trg_seq_len, trg_seq_len).\n",
        "\n",
        "        Returns:\n",
        "        - Tensor: Decoded tensor of shape (trg_seq_len, batch_size, emb_size).\n",
        "        \"\"\"\n",
        "        return self.transformer_decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ],
      "metadata": {
        "id": "GXW6kvCFRkjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Embedding and Embedding"
      ],
      "metadata": {
        "id": "bqueouy1hbyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Positional encoding module for adding positional information to token embeddings.\n",
        "\n",
        "    Args:\n",
        "    - emb_size (int): Embedding size.\n",
        "    - dropout (float): Dropout probability.\n",
        "    - maxlen (int): Maximum sequence length.\n",
        "\n",
        "    Methods:\n",
        "    - forward(token_embedding: Tensor):\n",
        "      Forward pass of the positional encoding module.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_size: int, dropout, maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # Calculate positional encodings\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Register positional embeddings as a buffer\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        \"\"\"\n",
        "        Forward pass of the positional encoding module.\n",
        "\n",
        "        Args:\n",
        "        - token_embedding (Tensor): Token embeddings of shape (seq_len, batch_size, emb_size).\n",
        "\n",
        "        Returns:\n",
        "        - Tensor: Token embeddings with positional encodings added, of shape (seq_len, batch_size, emb_size).\n",
        "        \"\"\"\n",
        "        # Add positional encodings to token embeddings\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0),:])\n",
        "\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Token embedding module for converting token indices into token embeddings.\n",
        "\n",
        "    Args:\n",
        "    - vocab_size (int): Vocabulary size.\n",
        "    - emb_size (int): Embedding size.\n",
        "\n",
        "    Methods:\n",
        "    - forward(tokens: Tensor):\n",
        "      Forward pass of the token embedding module.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        # Initialize an embedding layer for tokens\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        \"\"\"\n",
        "        Forward pass of the token embedding module.\n",
        "\n",
        "        Args:\n",
        "        - tokens (Tensor): Token indices of shape (seq_len, batch_size).\n",
        "\n",
        "        Returns:\n",
        "        - Tensor: Token embeddings of shape (seq_len, batch_size, emb_size).\n",
        "        \"\"\"\n",
        "        # Convert token indices into token embeddings and scale them\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
      ],
      "metadata": {
        "id": "F_Pfq1h6RpOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Masking"
      ],
      "metadata": {
        "id": "KjwaGEnGhlRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    \"\"\"\n",
        "    Generates a square subsequent mask for self-attention mechanisms in transformer models.\n",
        "\n",
        "    Args:\n",
        "    - sz (int): Size of the square mask.\n",
        "\n",
        "    Returns:\n",
        "    - Tensor: Square subsequent mask of shape (sz, sz).\n",
        "    \"\"\"\n",
        "    # Create an upper triangular matrix of ones\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    # Convert the mask to float and replace zeros with negative infinity and ones with zero\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    \"\"\"\n",
        "    Creates masks for source and target sequences to be used in transformer models.\n",
        "\n",
        "    Args:\n",
        "    - src (Tensor): Source tensor of shape (src_seq_len, batch_size).\n",
        "    - tgt (Tensor): Target tensor of shape (tgt_seq_len, batch_size).\n",
        "\n",
        "    Returns:\n",
        "    - src_mask (Tensor): Mask for source sequence of shape (src_seq_len, src_seq_len).\n",
        "    - tgt_mask (Tensor): Mask for target sequence of shape (tgt_seq_len, tgt_seq_len).\n",
        "    - src_padding_mask (Tensor): Padding mask for source sequence of shape (batch_size, src_seq_len).\n",
        "    - tgt_padding_mask (Tensor): Padding mask for target sequence of shape (batch_size, tgt_seq_len).\n",
        "    \"\"\"\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    # Generate mask for the target sequence\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    # Create a padding mask for the source sequence\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n",
        "\n",
        "    # Create padding masks for both source and target sequences\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "metadata": {
        "id": "meVp0AB5RsxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Important Parameters"
      ],
      "metadata": {
        "id": "pXuE_RxghqgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define vocabulary sizes and model parameters\n",
        "SRC_VOCAB_SIZE = len(en_vocab)\n",
        "TGT_VOCAB_SIZE = len(fr_vocab)\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Determine the device (CPU or GPU) for computation\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Instantiate the Seq2SeqTransformer model\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS,\n",
        "                                 EMB_SIZE, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE,\n",
        "                                 FFN_HID_DIM)\n",
        "\n",
        "# Initialize model parameters using Xavier initialization\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "# Move the model to the selected device\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "# Define the loss function (CrossEntropyLoss) ignoring padding tokens\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "# Define the optimizer (Adam) with specific parameters\n",
        "optimizer = torch.optim.Adam(\n",
        "    transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unO0IamRRw_W",
        "outputId": "485a01db-1a10-4ab3-dd62-d00aa2d0e04f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train function"
      ],
      "metadata": {
        "id": "D8fX141fln82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, train_iter, optimizer, epoch, save_path):\n",
        "    \"\"\"\n",
        "    Trains the model for one epoch using the provided training iterator and optimizer.\n",
        "\n",
        "    Args:\n",
        "    - model (nn.Module): The model to be trained.\n",
        "    - train_iter (DataLoader): The data loader iterator for training data.\n",
        "    - optimizer (torch.optim.Optimizer): The optimizer used for training.\n",
        "    - epoch (int): The current epoch number.\n",
        "    - save_path (str): The directory path to save the trained model.\n",
        "\n",
        "    Returns:\n",
        "    - train_loss (float): The average loss of the model over the train set.\n",
        "    \"\"\"\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loss = 0  # Initialize total loss for the epoch\n",
        "    total_correct = 0  # Initialize total number of correct predictions\n",
        "    total_elements = 0  # Initialize total token count for accuracy calculation\n",
        "\n",
        "    # Iterate over the training data iterator\n",
        "    for idx, (src, tgt) in enumerate(train_iter):\n",
        "        src = src.to(device)  # Move source data to the appropriate device\n",
        "        tgt = tgt.to(device)  # Move target data to the appropriate device\n",
        "        tgt_input = tgt[:-1, :]  # Get input to the decoder (exclude last token)\n",
        "\n",
        "        # Generate masks for source and target sequences\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        # Zero the gradients before backward pass\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Define target output (right-shifted)\n",
        "        tgt_out = tgt[1:, :]\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "\n",
        "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the total loss for the epoch\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate average loss and accuracy for the epoch\n",
        "    train_loss = total_loss / len(train_iter)\n",
        "\n",
        "    # Save the trained model after each epoch\n",
        "    model_save_path = f'{save_path}/model_epoch_{epoch}.pth'\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "    return train_loss"
      ],
      "metadata": {
        "id": "UO8uDhisR3L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate function"
      ],
      "metadata": {
        "id": "cVG2ZMh0lfbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, val_iter, device):\n",
        "    \"\"\"\n",
        "    Evaluate the performance of a model on the validation dataset.\n",
        "\n",
        "    This function calculates the average loss and accuracy of the model\n",
        "    over the entire validation set. The model is set to evaluation mode to\n",
        "    disable operations like dropout.\n",
        "\n",
        "    Args:\n",
        "    - model (torch.nn.Module): The model to be evaluated.\n",
        "    - val_iter (iterable): An iterable over the validation dataset. Each iteration\n",
        "                           produces a pair (src, tgt) representing source and target data.\n",
        "    - device (torch.device): The device tensors should be sent to (e.g., 'cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "    - val_loss (float): The average loss of the model over the validation set.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    total_elements = 0\n",
        "\n",
        "    for idx, (src, tgt) in enumerate(val_iter):\n",
        "        src = src.to(device)  # Move source data to the specified device\n",
        "        tgt = tgt.to(device)  # Move target data to the specified device\n",
        "\n",
        "        tgt_input = tgt[:-1, :]  # Prepare the input for the target sequence\n",
        "\n",
        "        # Create masks for the source and target input\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        # Forward pass: compute predicted logits by passing src, tgt_input, and masks to the model\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,\n",
        "                       src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]  # The actual targets, excluding the first element of tgt\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        total_loss += loss.item()  # Accumulate the loss\n",
        "\n",
        "    val_loss = total_loss / len(val_iter)  # Calculate average loss\n",
        "\n",
        "    return val_loss"
      ],
      "metadata": {
        "id": "VItaiiahR7hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Val loss"
      ],
      "metadata": {
        "id": "2IggiFOuljAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path where the model or other data might be saved or accessed.\n",
        "path = '/content/drive/MyDrive/trf/'\n",
        "\n",
        "# Loop over each epoch starting from 1 to NUM_EPOCHS\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = time.time()  # Record the start time of the epoch\n",
        "\n",
        "    # Train the model for one epoch and retrieve training loss and accuracy\n",
        "    train_loss = train_epoch(transformer, train_iter, optimizer, epoch, path)\n",
        "\n",
        "    end_time = time.time()  # Record the end time of the training\n",
        "\n",
        "    # Evaluate the model on the validation dataset and retrieve loss and accuracy\n",
        "    val_loss = evaluate(transformer, valid_iter, device)\n",
        "\n",
        "    # Print the results for the epoch including both training and validation metrics\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"\n",
        "           f\"Epoch time = {(end_time - start_time):.3f}s\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYUI7y_-SAaV",
        "outputId": "b2408242-0e32-4f04-d085-241903248d1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train loss: 3.249, Val loss: 2.723, Epoch time = 20.590s\n",
            "Epoch: 2, Train loss: 2.522, Val loss: 2.206, Epoch time = 20.501s\n",
            "Epoch: 3, Train loss: 2.081, Val loss: 1.908, Epoch time = 20.369s\n",
            "Epoch: 4, Train loss: 1.783, Val loss: 1.713, Epoch time = 20.305s\n",
            "Epoch: 5, Train loss: 1.561, Val loss: 1.576, Epoch time = 20.377s\n",
            "Epoch: 6, Train loss: 1.391, Val loss: 1.479, Epoch time = 20.530s\n",
            "Epoch: 7, Train loss: 1.254, Val loss: 1.413, Epoch time = 20.515s\n",
            "Epoch: 8, Train loss: 1.140, Val loss: 1.357, Epoch time = 20.435s\n",
            "Epoch: 9, Train loss: 1.045, Val loss: 1.313, Epoch time = 20.418s\n",
            "Epoch: 10, Train loss: 0.964, Val loss: 1.263, Epoch time = 20.337s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Greedy Decoder"
      ],
      "metadata": {
        "id": "c-T8j_OxlXRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    \"\"\"\n",
        "    Perform greedy decoding on the output of a model.\n",
        "\n",
        "    This function decodes a source input sentence into a target sentence using\n",
        "    the greedy approach where the most probable next word is chosen at each step.\n",
        "\n",
        "    Args:\n",
        "    - model (torch.nn.Module): The transformer model to use for decoding.\n",
        "    - src (torch.Tensor): The input tensor containing the source sentence indices.\n",
        "    - src_mask (torch.Tensor): The mask tensor for the source input, preventing\n",
        "                               the model from attending to padding positions.\n",
        "    - max_len (int): The maximum length of the output sentence to generate.\n",
        "    - start_symbol (int): The index of the start symbol used to initiate decoding.\n",
        "\n",
        "    Returns:\n",
        "    - torch.Tensor: The tensor containing the indices of the generated sentence.\n",
        "    \"\"\"\n",
        "\n",
        "    # Move the input source sentence and its mask to the same device as the model.\n",
        "    src = src.to(device)\n",
        "    src_mask = src_mask.to(device)\n",
        "\n",
        "    # Encode the source sentence and initialize the target with the start symbol.\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
        "\n",
        "    # Iterate to generate each symbol until max_len is reached or EOS is predicted.\n",
        "    for i in range(max_len-1):\n",
        "        # Create a subsequent mask for the target to prevent attention to future positions.\n",
        "        memory = memory.to(device)\n",
        "        memory_mask = torch.zeros(ys.shape[0], memory.shape[0]).to(device).type(torch.bool)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                                    .type(torch.bool)).to(device)\n",
        "\n",
        "        # Decode using the latest target input, the encoded memory, and the masks.\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "\n",
        "        # Select the token with the highest probability as the next token.\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        # Append the predicted word to the target sequence.\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "\n",
        "        # If the end of sentence (EOS) token is predicted, stop decoding.\n",
        "        if next_word == EOS_IDX:\n",
        "          break\n",
        "\n",
        "    return ys"
      ],
      "metadata": {
        "id": "2-VyrPZWSFkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translation function"
      ],
      "metadata": {
        "id": "p-ISl_C2lTQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(model, src, src_vocab, tgt_vocab, src_tokenizer):\n",
        "    \"\"\"\n",
        "    Translate a source sentence into the target language using a given model.\n",
        "\n",
        "    This function tokenizes the input sentence, encodes it into indices, performs\n",
        "    greedy decoding to generate the output sequence, and then converts the output\n",
        "    indices back to text.\n",
        "\n",
        "    Args:\n",
        "    - model (torch.nn.Module): The trained translation model.\n",
        "    - src (str): The source sentence to translate.\n",
        "    - src_vocab (Vocab): The source vocabulary object that maps tokens to indices.\n",
        "    - tgt_vocab (Vocab): The target vocabulary object that maps indices to tokens.\n",
        "    - src_tokenizer (function): The tokenizer function for the source language.\n",
        "\n",
        "    Returns:\n",
        "    - str: The translated sentence in the target language.\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()  # Set the model to evaluation mode to disable training-specific behaviors such as dropout.\n",
        "\n",
        "    # Tokenize the input sentence, convert tokens to indices, and add boundary tokens.\n",
        "    tokens = [BOS_IDX] + [src_vocab.get_stoi()[tok] for tok in src_tokenizer(src)] + [EOS_IDX]\n",
        "    num_tokens = len(tokens)\n",
        "    src = torch.LongTensor(tokens).reshape(num_tokens, 1)  # Reshape for model input.\n",
        "\n",
        "    # Create a source mask (assuming full attention across the input).\n",
        "    src_mask = torch.zeros(num_tokens, num_tokens).type(torch.bool)\n",
        "\n",
        "    # Decode the source input to generate target tokens.\n",
        "    tgt_tokens = greedy_decode(model, src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "\n",
        "    # Convert the output indices to tokens and join into a single string.\n",
        "    # Remove special tokens <bos> and <eos> for final output.\n",
        "    return \" \".join([tgt_vocab.get_itos()[tok] for tok in tgt_tokens]).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ],
      "metadata": {
        "id": "j6FiG6fuSLjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample output"
      ],
      "metadata": {
        "id": "sSebYgWIlP7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Translate an English sentence into French using a pretrained transformer model.\n",
        "# 'transformer' is the model, 'en_vocab' is the English vocabulary, 'fr_vocab' is the French vocabulary,\n",
        "# and 'en_tokenizer' is the tokenizer function for English.\n",
        "output = translate(transformer, \"A group of people talking.\", en_vocab, fr_vocab, en_tokenizer)\n",
        "\n",
        "# Print the translated sentence. The expected output should be the French translation\n",
        "# of the English sentence \"A group of people talking.\"\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-U2m8MSLSQuW",
        "outputId": "b54ade8f-3366-4e0f-88c4-a63fdedd6de5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Un groupe de personnes parlant . \n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = translate(transformer, \"He sings in the choir.\", en_vocab, fr_vocab, en_tokenizer)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlAwjWoCSdE9",
        "outputId": "8ab869e1-17dd-4901-8728-5cf95ce13474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Il chante dans la chorale . \n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DViaYZne7rTu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}